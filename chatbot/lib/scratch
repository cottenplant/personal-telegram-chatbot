def main():
    # read in preprocess and create token sequences
    message_text = read_file(paths_local['chats_processed_txt'])
    tokenized_text = preprocess(message_text)
    tokenizer, sequences, text_sequences = make_tokenized_sequence(tokenized_text)

    # train, test, split, fot
    model = fit_model(tokenizer, sequences)
    random_seed_text = pick_random_seed_text(text_sequences)

    # generate some wisdom
    words_of_wisdom = []
    for _ in range(100):
        result = generate_text(
            model=model,
            tokenizer=tokenizer,
            seq_len=paths_local['seq_len'],
            seed_text=random_seed_text,
            num_gen_words=paths_local['num_gen_words']
        )
        words_of_wisdom.append(result)
    print(words_of_wisdom)
    pd.DataFrame(words_of_wisdom).to_csv(paths_local['generated_csv'], index=False)


